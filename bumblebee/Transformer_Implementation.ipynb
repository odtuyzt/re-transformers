{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Implementation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpvgaY_9VKL6",
        "outputId": "dca070d3-d66b-4f23-bb60-490ef102fd26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 14.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 22.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed torchdata-0.3.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from copy import copy"
      ],
      "metadata": {
        "id": "1WffB0H1cHod"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "f = open(\"parallelcorpus.txt\", \"a\")\n",
        "\n",
        "for i in train_iter:\n",
        "  for x in [x.rstrip(\"\\n\") for x in i]:\n",
        "    f.write(x)\n",
        "    f.write(' ')\n",
        "\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "GQVFexHPCQt-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 64\n",
        "VOCAB_SIZE = 32768\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train(['parallelcorpus.txt'], trainer)\n",
        "\n",
        "tokenizer.enable_padding(pad_id=1, length=MAX_LEN)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
        "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_enc = tokenizer.encode(src_sample.rstrip(\"\\n\"))\n",
        "        src_batch.append(torch.tensor(src_enc.ids))\n",
        "\n",
        "        tgt_enc = tokenizer.encode(tgt_sample.rstrip(\"\\n\"))\n",
        "        tgt_batch.append(torch.tensor(tgt_enc.ids))\n",
        "\n",
        "\n",
        "\n",
        "    return torch.stack(src_batch), torch.stack(tgt_batch)"
      ],
      "metadata": {
        "id": "pRcQKR0scNow"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, BATCH_SIZE, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "-OWJTwb2vod0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, pad_mask):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x)"
      ],
      "metadata": {
        "id": "7GYWcPNjd1Ua"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_ff = d_ff\n",
        "        self.d_k = self.d_v = int(d_model / d_h)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.concatLinear = nn.Linear(d_model, d_model, bias=False) # Linear Layer for the concatenated head\n",
        "        self.normalize = nn.LayerNorm(d_model) # Normalizing Layer\n",
        "\n",
        "        self.feed_forward = nn.Sequential( # Feed Forward Layer\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(d_h*3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = []\n",
        "        for i in range(self.d_h):\n",
        "            Q = self.linears[3*i](x) # Query Matrix\n",
        "            K = self.linears[3*i + 1](x) # Key Matrix\n",
        "            V = self.linears[3*i + 2](x) # Value Matrix\n",
        "\n",
        "            scaledMatMul = Q @ K.transpose(-1, -2) / math.sqrt(self.d_k) # MatMul of Q and K -> Scale\n",
        "\n",
        "            head = F.softmax(scaledMatMul) @ V # SoftMax -> MatMul of Q,K and V\n",
        "\n",
        "            heads.append(head) # A Single Head\n",
        "\n",
        "        Z = self.concatLinear(torch.cat((heads), -1)) # Concatenated heads -> Linear Layer\n",
        "\n",
        "        Z = self.dropout(Z)\n",
        "\n",
        "        AddNorm = self.normalize(x + Z) # Output of the First Add&Norm Layer\n",
        "        \n",
        "        Z = self.normalize(self.feed_forward(AddNorm) + AddNorm) # 1st Add&Norm -> Feed Forward -> 2nd Add&Norm\n",
        "\n",
        "        return Z"
      ],
      "metadata": {
        "id": "71IIvzmUYYtE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_h, dropout, N):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        self.encoders = nn.ModuleList([EncoderLayer(d_model, d_ff, d_h, dropout) for _ in range(N)]) # Stacking Encoder Layer N Times\n",
        "\n",
        "    def forward(self, x):\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Cg67GeB7imYg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1): # parameters={}\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_k = self.d_v = int(self.d_model / self.d_h)\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(d_h*3)]) # Linear Layers\n",
        "\n",
        "        self.firstLinear = nn.Linear(d_h * self.d_v, d_model, bias=False) # Linear Layer for the Concatenated Head\n",
        "\n",
        "        self.secondLinear = nn.Linear(d_h * self.d_model, d_model, bias=False) # Linear Layer for the Concatenated Head(second multi-head attention)\n",
        "\n",
        "        self.normalize = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.mask = torch.triu( # Lower Triangular Mask Matrix\n",
        "            torch.tensor([[[-np.inf for _ in range(self.d_k)] for _ in range(self.d_k)] for _ in range(16)]), diagonal=1\n",
        "        )\n",
        "\n",
        "        self.feed_forward = nn.Sequential( # Feed Forward Layer\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        heads1 = []\n",
        "        heads2 = []\n",
        "        for i in range(self.d_h):\n",
        "\n",
        "            # FIRST ATTENTION LAYER OF THE DECODER\n",
        "            ''' Same as decoder, but here we have tgt(target) as the decoder's input. '''\n",
        "            Q = self.linears[3*i](y) # Query Matrix\n",
        "            K = self.linears[3*i+1](y) # Key Matrix\n",
        "            V = self.linears[3*i+2](y) # Value Matrix\n",
        "            \n",
        "            scaledMatMul = Q @ K.transpose(-1, -2) / math.sqrt(self.d_k) # Matrix Multiplication of Q and K -> Scale\n",
        "            maskedMat = scaledMatMul + self.mask # Masking, optional\n",
        "            soft = F.softmax(scaledMatMul) # SoftMax\n",
        "\n",
        "            head =  soft @ V # Matrix Multiplication of Scaled and Soft Maxed Q, K Matrices with V\n",
        "\n",
        "            heads1.append(head) # Appending a Single Head\n",
        "\n",
        "        Z1 = self.firstLinear(torch.cat((heads1), dim=-1)) # Concatenated Heads of the First Attention Layer\n",
        "\n",
        "        AddNorm1 = self.normalize(x + Z1) # First Normalizing Layer\n",
        "\n",
        "        for i in range(self.d_h): # Second Attention Layer\n",
        "\n",
        "            # SECOND ATTENTION LAYER OF THE DECODER\n",
        "            ''' A typical Attention layer, however, instead of Q and K matrices, we use the output of the encoder. '''\n",
        "            scaledMat = x @ x.transpose(-1, -2) / math.sqrt(self.d_k) # Matrix Multiplication of SRC and it's Transpose -> Scale\n",
        "\n",
        "            soft = F.softmax(scaledMat) # Soft Max\n",
        "\n",
        "            head = soft @ Z1 # Matrix Multiplication of Scaled and Soft Maxed X, X.T and the Output of the Previous Multi Head Attention Layer\n",
        "\n",
        "            heads2.append(head) # Appending a Single Head\n",
        "\n",
        "        Z2 = self.secondLinear(torch.cat((heads2), dim=-1)) # Concatenated Heads of the Second Attention Layer\n",
        "\n",
        "        AddNorm2 = self.normalize(AddNorm1 + Z2) # Second Normalizing Layer -- The Output of the First Normalizing Layer is Being Used\n",
        "\n",
        "        Z = self.normalize(AddNorm2 + self.feed_forward(AddNorm2)) # Feed Forward -> Normalize\n",
        "\n",
        "        return Z"
      ],
      "metadata": {
        "id": "8Gra_PyaikDJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_h, dropout, N):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.decoders = nn.ModuleList([DecoderLayer(d_model, d_ff, d_h) for _ in range(N)]) # Stacking Decoder Layer N Times\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        for decoder in self.decoders:\n",
        "            y = decoder(x, y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "C2VGTD8-ikcT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size=512, d_model=512, d_h = 8, d_ff=2048, vocab_size=32768, dropout=0.1, num_coder_layers=6):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False) # Final Linear Layer of Our Model\n",
        "        self.dropoutEnc = nn.Dropout(dropout)\n",
        "        self.dropoutDec = nn.Dropout(dropout)\n",
        "        self.softmax = F.softmax\n",
        "        self.embed = Embedding(vocab_size, embedding_size, pad_mask=1)\n",
        "        self.encoderStack = EncoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "        self.decoderStack = DecoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "\n",
        "    def PositionalEncoding(self, seq_len, d_model, n):\n",
        "        P = torch.zeros(seq_len, d_model)\n",
        "        for k in range(seq_len):\n",
        "            for i in range(int(d_model/2)):\n",
        "                denominator = math.pow(n, 2*i/d_model)\n",
        "                P[k, 2*i] = math.sin(k/denominator)\n",
        "                P[k, 2*i+1] = math.cos(k/denominator)\n",
        "        return P\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = (self.embed(x))\n",
        "\n",
        "        y = (self.embed(y))\n",
        "\n",
        "        pos_encoding = self.PositionalEncoding(self.d_model//self.d_h, self.d_model, 10000)\n",
        "\n",
        "        x = pos_encoding + x\n",
        "\n",
        "        y = pos_encoding + y\n",
        "\n",
        "        x = self.dropoutEnc(x)\n",
        "\n",
        "        y = self.dropoutDec(y)\n",
        "\n",
        "        encoderOutput = self.encoderStack(x.float())\n",
        "\n",
        "        decoderOutput = self.decoderStack(encoderOutput, y.float())\n",
        "\n",
        "        logits = self.linear(decoderOutput)\n",
        "\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "        return probs"
      ],
      "metadata": {
        "id": "ULn1jcmkijB4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(embedding_size=512, d_model=512, d_h=8, d_ff=2048, vocab_size=32768, dropout=0.1, num_coder_layers=6)"
      ],
      "metadata": {
        "id": "3YozcXkE6mmL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "HMbEKbJESGwd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss = 0.0\n",
        "i=0\n",
        "for epoch in range(2):\n",
        "    for source, target in train_dataloader:\n",
        "        if i==1812:\n",
        "            break\n",
        "        input, labels = source, target\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        unchangedLabels = labels.detach().clone()\n",
        "\n",
        "        outputs = torch.argmax(model(input, labels), dim=2)\n",
        "\n",
        "        outputs = outputs.reshape(1, 1024)\n",
        "\n",
        "        unchangedLabels = unchangedLabels.reshape(1, 1024)\n",
        "\n",
        "        loss = criterion(torch.tensor(outputs, dtype=torch.float, requires_grad=True), torch.tensor(unchangedLabels, dtype=torch.float, requires_grad=True))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        i=i+1\n",
        "        if i % 10 == 1:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss}')\n",
        "            running_loss = 0.0\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "4sqWOEjBSG88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}