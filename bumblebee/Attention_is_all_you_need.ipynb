{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NNFNJRrv9pQB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, pad_mask):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_ff = d_ff\n",
        "        self.d_k = self.d_v = int(d_model / d_h)\n",
        "\n",
        "        self.concatLinear = nn.Linear(d_model, d_model, bias=False) # Linear Layer for the concatenated head\n",
        "        self.normalize = nn.LayerNorm(d_model) # Normalizing Layer\n",
        "\n",
        "        self.feed_forward = nn.Sequential( # Feed Forward Layer\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(d_h*3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = []\n",
        "        for i in range(self.d_h):\n",
        "            Q = self.linears[3*i](x) # Query Matrix\n",
        "            K = self.linears[3*i + 1](x) # Key Matrix\n",
        "            V = self.linears[3*i + 2](x) # Value Matrix\n",
        "\n",
        "            scaledMatMul = torch.matmul(Q, K.transpose(-1,-2)) / math.sqrt(self.d_k) # MatMul of Q and K -> Scale\n",
        "\n",
        "            scaledMatMul[[scaledMatMul==1]] = -np.inf # Masking the Padding Indexes With Minus Infinity\n",
        "\n",
        "            head = torch.matmul(F.softmax(scaledMatMul), V) # SoftMax -> MatMul of Q,K and V\n",
        "\n",
        "            heads.append(head) # A Single Head\n",
        "\n",
        "        Z = self.concatLinear(torch.cat((heads), -1)) # Concatenated heads -> Linear Layer\n",
        "\n",
        "        AddNorm = self.normalize(x + Z) # Output of the First Add&Norm Layer\n",
        "        \n",
        "        Z = self.normalize(self.feed_forward(AddNorm) + AddNorm) # 1st Add&Norm -> Feed Forward -> 2nd Add&Norm\n",
        "\n",
        "        return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_h, N):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        self.encoders = nn.ModuleList([EncoderLayer(d_model, d_ff, d_h) for _ in range(N)]) # Stacking Encoder Layer N Times\n",
        "\n",
        "    def forward(self, src):\n",
        "        for encoder in self.encoders:\n",
        "            src = encoder(src)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, batch_size=16): # parameters={}\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_k = self.d_v = int(self.d_model / self.d_h)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k, bias=False) for _ in range(d_h*3)]) # Linear Layers\n",
        "\n",
        "        self.firstLinear = nn.Linear(d_h * self.d_v, d_model, bias=False) # Linear Layer for the Concatenated Head\n",
        "\n",
        "        self.secondLinear = nn.Linear(d_h * self.d_model, d_model, bias=False) # Linear Layer for the Concatenated Head(second multi-head attention)\n",
        "\n",
        "        self.normalize = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.mask = torch.triu( # Lower Triangular Mask Matrix\n",
        "            torch.tensor([[[-np.inf for _ in range(self.d_k)] for _ in range(self.d_k)] for _ in range(batch_size)]), diagonal=1\n",
        "        )\n",
        "\n",
        "        self.feed_forward = nn.Sequential( # Feed Forward Layer\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        heads1 = []\n",
        "        heads2 = []\n",
        "        for i in range(self.d_h):\n",
        "\n",
        "            # FIRST ATTENTION LAYER OF THE DECODER\n",
        "            ''' Same as decoder, but here we have tgt(target) as the decoder's input. '''\n",
        "            Q = self.linears[3*i](tgt) # Query Matrix\n",
        "            K = self.linears[3*i+1](tgt) # Key Matrix\n",
        "            V = self.linears[3*i+2](tgt) # Value Matrix\n",
        "            \n",
        "            scaledMatMul = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k) # Matrix Multiplication of Q and K -> Scale\n",
        "\n",
        "            # maskedMat = scaledMatMul + self.mask # Masking, optional\n",
        "\n",
        "            soft = F.softmax(scaledMatMul) # SoftMax\n",
        "\n",
        "            head = torch.matmul(soft, V) # Matrix Multiplication of Scaled and Soft Maxed Q, K Matrices with V\n",
        "\n",
        "            heads1.append(head) # Appending a Single Head\n",
        "\n",
        "        Z1 = self.firstLinear(torch.cat((heads1), dim=-1)) # Concatenated Heads of the First Attention Layer\n",
        "\n",
        "        AddNorm1 = self.normalize(src + Z1) # First Normalizing Layer\n",
        "\n",
        "        for i in range(self.d_h): # Second Attention Layer\n",
        "\n",
        "            # SECOND ATTENTION LAYER OF THE DECODER\n",
        "            ''' A typical Attention layer, however, instead of Q and K matrices, we use the output of the encoder. '''\n",
        "            scaledMat = torch.matmul(src, src.transpose(-1, -2)) / math.sqrt(self.d_k) # Matrix Multiplication of SRC and it's Transpose -> Scale\n",
        "\n",
        "            soft = F.softmax(scaledMat) # Soft Max\n",
        "\n",
        "            head = torch.matmul(soft, Z1) # Matrix Multiplication of Scaled and Soft Maxed X, X.T and the Output of the Previous Multi Head Attention Layer\n",
        "\n",
        "            heads2.append(head) # Appending a Single Head\n",
        "\n",
        "        Z2 = self.secondLinear(torch.cat((heads2), dim=-1)) # Concatenated Heads of the Second Attention Layer\n",
        "\n",
        "        AddNorm2 = self.normalize(AddNorm1 + Z2) # Second Normalizing Layer -- The Output of the First Normalizing Layer is Being Used\n",
        "\n",
        "        Z = self.normalize(AddNorm2 + self.feed_forward(AddNorm2)) # Feed Forward -> Normalize\n",
        "\n",
        "        return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, d_h, batch_size, N):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.decoders = nn.ModuleList([DecoderLayer(d_model, d_ff, d_h, batch_size) for _ in range(N)]) # Stacking Decoder Layer N Times\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        for decoder in self.decoders:\n",
        "            src = decoder(src, tgt)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, batch_size=16, embedding_size=512, d_model=512, d_h = 8, d_ff=2048, vocab_size=32768, num_coder_layers=6):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.linear = nn.Linear(d_model, d_model, bias=False) # Final Linear Layer of Our Model\n",
        "\n",
        "        ''' Instead of using a new linear layer for the final linear layer of our model\n",
        "        the paper suggests that we use the shared weights of the input and the output embeddings. '''\n",
        "\n",
        "        self.softmax = F.softmax\n",
        "        self.embed = Embedding(vocab_size, embedding_size, pad_mask=1)\n",
        "        self.encoderStack = EncoderStack(d_model, d_ff, d_h, num_coder_layers)\n",
        "        self.decoderStack = DecoderStack(d_model, d_ff, d_h, batch_size, num_coder_layers)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        srcEmbedded = self.embed(src)\n",
        "\n",
        "        tgtEmbedded = self.embed(tgt)\n",
        "\n",
        "        encoderOutput = self.encoderStack(srcEmbedded)\n",
        "\n",
        "        decoderOutput = self.decoderStack(encoderOutput, tgtEmbedded)\n",
        "\n",
        "        print(self.embed.emb.weight.t().shape)\n",
        "\n",
        "        logits = torch.matmul(torch.sum(decoderOutput, dim=1), self.embed.emb.weight.t())\n",
        "\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "        return probs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Attention is all you need.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
