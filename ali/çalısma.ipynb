{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZn19XYpryGl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import math\n",
        "class multihead(nn.Module):\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    super(multihead, self).__init__()\n",
        "    assert d_model % h == 0\n",
        "    self.d_k = d_model // h\n",
        "    self.h = h\n",
        "    self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "    self.attn = None\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "      liste = [q, k, v]\n",
        "      nbatches = q.size(0)\n",
        "      for i in range(len(liste)):\n",
        "        for lin, x in zip(self.linears, liste[i]):\n",
        "          liste[i] = lin(x)\n",
        "\n",
        "      q = liste[0]\n",
        "      k = liste[1]\n",
        "      v = liste[2]\n",
        "\n",
        "      x = attention(q, k, v) \n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor(1,3)\n",
        "print(a.shape)\n",
        "b = a.transpose(-2,-1)\n",
        "print(b.shape)\n",
        "print(a.size(-1))\n",
        "c = nn.Linear(2,2)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xopO1O1547Zs",
        "outputId": "ecb68cc2-bcfa-4d16-a250-2c3dbfc10b08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3])\n",
            "torch.Size([3, 1])\n",
            "3\n",
            "Linear(in_features=2, out_features=2, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "WSkWLno8vM2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = [\n",
        "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for lin, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(\n",
        "            query, key, value, mask=mask, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = (\n",
        "            x.transpose(1, 2)\n",
        "            .contiguous()\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "        )\n",
        "        del query\n",
        "        del key\n",
        "        del value\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "zlw-jqpjt3Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "    \n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "oMe8LC-os_XV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}