# -*- coding: utf-8 -*-
"""Transformer_Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gWnJwkZfOWfULlnUTzIyMnCjjrijEWrN
"""

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class EncoderLayer(nn.Module):
    def __init__(self,feedforward,attention,layernorm):
        self.norm = layernorm
        self.attention = attention
        self.feedforward = feedforward

    def forward(self,x):
        temp = self.layernorm(self.attention(x,x,x),x)
        return self.norm(temp,self.feedforward(temp))

class EncoderStack(nn.Module):
    def __init__(self,N=6,encoderLayer):
        super(EncoderStack, self)._init_()
        self.stack = clone(encoderLayer,N)

    def forward(self,x):
        for encoderLayer in stack:
            x = encoderLayer(x)
        return x

class MultiHeadAttention(nn.Module):
    def _init_(self,d_model,h):
        super(MultiHeadAttention, self)._init_()
        self.flatten = nn.Flatten()
        self.linears = nn.ModuleList([nn.Linear(d_model,d_model),nn.Linear(d_model,d_model),nn.Linear(d_model,d_model),nn.Linear(d_model,d_model)])
        self.dk = d_model // h
        after_linear = list()
        )

    def Scaled_Attention(q,k,v):
      nominator = torch.matmul(q,torch.transpose(k,0,1))
      denominator = (k.shape[1]**(1/2))
      var1 = torch.nn.functional.softmax((nominator/denominator))     
      return torch.matmul(var1,v)

    def forward(self, queries, keys, values):
        parameters = [queries,keys,values]
        for i in range(3):
          after_linear.append(self.linears[i](parameters[i]))
        x = Scaled_Attention(after_linear[0],after_linear[1],after_linear[2])
        return self.linears[-1](x)

class Add_Norm(nn.Module):
    def __init__(self):
      pass

    def forward(self,x1,x2):
      layer_norm = torch.nn.LayerNorm(x1.size)
      return layer_norm((x1+x2))

class FeedForward(nn.Module):
    def _init_(self,d_model,d_ff):
        super(NeuralNetwork, self)._init_()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )

    def forward(self,x):
        x = linear_relu_stack(x)
        return x

